# ProjectMarloweNewDataset

I'm not very satisfied with the existing open source subjectivity/objectivity dataset. It comes from movie reviews and provides 5000 sentences objective and 5000 subjective sentences.  There isn't anything particularly wrong with it, but for the purposes of this project, I think a much larger dataset is required to improve the accuracy of the underlying machine learning microservices that are trained on it.  After this new dataset is setup, I can setup a third machine learning microservice that uses the new dataset to classify text and add it to existing Objectivty ensemble.

I want to systematically harvest to create a new objective/subjective dataset.  The reddit comments dataset is huge and probably a good place for subjective sentences, but I don't want to let a bunch of objective sentences slip through so I'm going to run any sentences I harvest from it through the existing classifiers I have setup.  Low scores can be added to the subjective data, middle scores will be excluded and high scores can go into the objective data.  Objective sentences are harder, but I think I can gather a goodly amount if I scrape some scientific journals and news sources and run them through the existing classifiers for high scoring sentences.

Ideally, a human can go over the data afterward and weed out any outliers, but that's a time intensive task that I can't commit to currently.  This is an open source project, so hopefully if anyone else out there uses it and is trying to help out they can remove bad data and send a pull request.



Downloading the 8 GB reddit comments json takes a little while.  It is 1.7 billion comments, now that's some real data, not a measely 10k.  I'm setting up a script in the dataset repository to handle parsing the comments and then checking their score with the existing services.  Going to query services locally with it and buildout a docker-compose.yml in the base devops repo that will spin up the two services on different ports and then call the reddit parse script to use those services.



Ok, parsed some comments, there are comments getting through as objective that I think are subjective, not the other way around.  I think its because these algorithms are ranking words, mis-spellings, and emojis as objective rather subjective and its wrong about that.  Part of the reason I'm building this dataset is to fix this error rate in the existing algorithms built off the movie review dataset that textblob is using and the dictionary approach that usent is using.  Going through these to fix what I can and going to scout a bit to see if I can find any volunteers interested in contributing by proofreading the objectivity dataset.

Once I've proof read a chunk of the comments, I'm going to build out the new microservice that will be trained off of this new dataset.  This part is going to be really easy, essentially going to copy the existing marlowe_objectivity microservice and modify it a little bit to use the objectivity dataset.  I'm sure any of you unfamiliar with microservices reading this may be thinking, copy blocks of code, oh no!!  That isn't DRY!  You are correct, but when dealing with microservices code re-use needs to be dealt with carefully.  There are components to the existing usent and objectivity microservices I've built where I duplicate code to both.  Code re-use in microservices is a major source of coupling and needs to be watched carefully.